{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/miniconda3/envs/DiscTorsionAnalyzer/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sam/miniconda3/envs/DiscTorsionAnalyzer/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/sam/miniconda3/envs/DiscTorsionAnalyzer/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/tmp/ipykernel_17174/3668618158.py:224: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 1: IDRiD_01.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/Dropbox/PROJECTS/DiscTorsionAnalyzer/utils.py:22: UserWarning: Only one label was provided to `remove_small_objects`. Did you mean to use a boolean array?\n",
      "  binary_image = morphology.remove_small_objects(labels, min_size=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image 2: IDRiD_02.png\n",
      "Processed image 3: IDRiD_03.png\n",
      "Processed image 4: IDRiD_04.png\n",
      "Processed image 5: IDRiD_05.png\n",
      "Processed image 6: IDRiD_06.png\n",
      "Processed image 7: IDRiD_07.png\n",
      "Processed image 8: IDRiD_08.png\n",
      "Processed image 9: IDRiD_09.png\n",
      "Processed image 10: IDRiD_10.png\n",
      "Processed image 11: IDRiD_11.png\n",
      "Processed image 12: IDRiD_12.png\n",
      "Processed image 13: IDRiD_13.png\n",
      "Processed image 14: IDRiD_14.png\n",
      "Processed image 15: IDRiD_15.png\n",
      "Processed image 16: IDRiD_16.png\n",
      "Processed image 17: IDRiD_17.png\n",
      "Processed image 18: IDRiD_18.png\n",
      "Processed image 19: IDRiD_19.png\n",
      "Processed image 20: IDRiD_20.png\n",
      "Processed image 21: IDRiD_21.png\n",
      "Processed image 22: IDRiD_22.png\n",
      "Processed image 23: IDRiD_23.png\n",
      "Processed image 24: IDRiD_24.png\n",
      "Processed image 25: IDRiD_25.png\n",
      "Processed image 26: IDRiD_26.png\n",
      "Processed image 27: IDRiD_27.png\n",
      "Processed image 28: IDRiD_28.png\n",
      "Processed image 29: IDRiD_29.png\n",
      "Processed image 30: IDRiD_30.png\n",
      "Processed image 31: IDRiD_31.png\n",
      "Processed image 32: IDRiD_32.png\n",
      "Processed image 33: IDRiD_33.png\n",
      "Processed image 34: IDRiD_34.png\n",
      "Processed image 35: IDRiD_35.png\n",
      "Processed image 36: IDRiD_36.png\n",
      "Processed image 37: IDRiD_37.png\n",
      "Processed image 38: IDRiD_38.png\n",
      "Processed image 39: IDRiD_39.png\n",
      "Processed image 40: IDRiD_40.png\n",
      "Processed image 41: IDRiD_41.png\n",
      "Processed image 42: IDRiD_42.png\n",
      "Processed image 43: IDRiD_43.png\n",
      "Processed image 44: IDRiD_44.png\n",
      "Processed image 45: IDRiD_45.png\n",
      "Processed image 46: IDRiD_46.png\n",
      "Processed image 47: IDRiD_47.png\n",
      "Processed image 48: IDRiD_48.png\n",
      "Processed image 49: IDRiD_49.png\n",
      "Processed image 50: IDRiD_50.png\n",
      "Processed image 51: IDRiD_51.png\n",
      "Processed image 52: IDRiD_52.png\n",
      "Processed image 53: IDRiD_53.png\n",
      "Processed image 54: IDRiD_54.png\n",
      "Processed image 55: IDRiD_55.png\n",
      "Processed image 56: IDRiD_56.png\n",
      "Processed image 57: IDRiD_57.png\n",
      "Processed image 58: IDRiD_58.png\n",
      "Processed image 59: IDRiD_59.png\n",
      "Processed image 60: IDRiD_60.png\n",
      "Processed image 61: IDRiD_61.png\n",
      "Processed image 62: IDRiD_62.png\n",
      "Processed image 63: IDRiD_63.png\n",
      "Processed image 64: IDRiD_64.png\n",
      "Processed image 65: IDRiD_65.png\n",
      "Processed image 66: IDRiD_66.png\n",
      "Processed image 67: IDRiD_67.png\n",
      "Processed image 68: IDRiD_68.png\n",
      "Processed image 69: IDRiD_69.png\n",
      "Processed image 70: IDRiD_70.png\n",
      "Processed image 71: IDRiD_71.png\n",
      "Processed image 72: IDRiD_72.png\n",
      "Processed image 73: IDRiD_73.png\n",
      "Processed image 74: IDRiD_74.png\n",
      "Processed image 75: IDRiD_75.png\n",
      "Processed image 76: IDRiD_76.png\n",
      "Processed image 77: IDRiD_77.png\n",
      "Processed image 78: IDRiD_78.png\n",
      "Processed image 79: IDRiD_79.png\n",
      "Processed image 80: IDRiD_80.png\n",
      "Processed image 81: IDRiD_81.png\n"
     ]
    }
   ],
   "source": [
    "# My imports\n",
    "from utils import *\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import mobilenet_v2\n",
    "from PIL import Image \n",
    "import configparser\n",
    "\n",
    "# Read the configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Set paths\n",
    "pathIn = config['paths']['pathIn']\n",
    "pathOut = config['paths']['pathOut']\n",
    "\n",
    "# Set the preprocessing parameters\n",
    "writeResults = config.getboolean('options', 'writeResults')\n",
    "writeImages = config.getboolean('options', 'writeImages')\n",
    "\n",
    "# Display processed images in notebook\n",
    "display = False\n",
    "\n",
    "# Sort the files in the image directory\n",
    "files = os.listdir(pathIn); files.sort()\n",
    "\n",
    "# Check if GPU is available and use it; otherwise, fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained MobileNetV2 model\n",
    "mobilenet_v2 = mobilenet_v2(pretrained=True)\n",
    "\n",
    "# Create a new Deeplabv3 model with a modified MobileNetV2 backbone\n",
    "class DeepLabV3MobileNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepLabV3MobileNetV2, self).__init__()\n",
    "        self.backbone = nn.Sequential(*list(mobilenet_v2.features.children()))\n",
    "        self.classifier = DeepLabHead(1280, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_shape = x.shape[-2:]\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        x = nn.functional.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n",
    "        return {'out': x}\n",
    "\n",
    "# Load disc model\n",
    "model_disc = torch.load('model_disc.pth')\n",
    "model_disc = model_disc.to(device) \n",
    "model_disc.eval()\n",
    "\n",
    "# Load fovea model\n",
    "model_fovea = torch.load('model_fovea.pth')\n",
    "model_fovea = model_fovea.to(device) \n",
    "model_fovea.eval()\n",
    "\n",
    "# Define a function to get the disc mask for an image\n",
    "def get_mask_for_disc(image):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    image_tensor = preprocess_disc(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        output = model_disc(image_tensor)\n",
    "    mask = output['out'].argmax(dim=1).squeeze().cpu().numpy()\n",
    "    return mask\n",
    "\n",
    "# Define a function to get the fovea mask for an image\n",
    "def get_mask_for_fovea(image):\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    image_tensor = preprocess_fovea(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        output = model_fovea(image_tensor)\n",
    "    mask = output['out'].argmax(dim=1).squeeze().cpu().numpy()\n",
    "    return mask\n",
    "\n",
    "# Prepare empty dataframe to store results\n",
    "results = pd.DataFrame(columns=['Filename', 'Ovality', 'Torsion'])\n",
    "\n",
    "# Process images  \n",
    "for i in range(len(files)):\n",
    "\n",
    "    try:\n",
    "\n",
    "        path = os.path.join(pathIn, files[i])  \n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (650, int(image.shape[0] * (650 / image.shape[1]))))\n",
    "        image = remove_black_border(image)\n",
    "\n",
    "        # Process masks\n",
    "        disc = cv2.resize(get_mask_for_disc(image).astype(np.uint8), (image.shape[1], image.shape[0]))\n",
    "        fovea = cv2.resize(get_mask_for_fovea(image).astype(np.uint8), (image.shape[1], image.shape[0]))\n",
    "\n",
    "        # Calculate the centroid of the disc mask\n",
    "        M_disc = cv2.moments(disc)\n",
    "        if M_disc[\"m00\"] != 0:\n",
    "            disc_center_x = int(M_disc[\"m10\"] / M_disc[\"m00\"])\n",
    "            disc_center_y = int(M_disc[\"m01\"] / M_disc[\"m00\"])\n",
    "        else:\n",
    "            disc_center_x = 0\n",
    "            disc_center_y = 0\n",
    "\n",
    "        # Calculate the centroid of the fovea mask\n",
    "        M_fovea = cv2.moments(fovea)\n",
    "        if M_fovea[\"m00\"] != 0:\n",
    "            fovea_center_x = int(M_fovea[\"m10\"] / M_fovea[\"m00\"])\n",
    "            fovea_center_y = int(M_fovea[\"m01\"] / M_fovea[\"m00\"])\n",
    "        else:\n",
    "            fovea_center_x = 0\n",
    "            fovea_center_y = 0\n",
    "\n",
    "        # Calculate the angle to rotate\n",
    "        dy = fovea_center_y - disc_center_y\n",
    "        dx = fovea_center_x - disc_center_x\n",
    "        angle = np.arctan(dy / dx) * (180 / np.pi)\n",
    "\n",
    "        # Calculate the center of the image for rotation\n",
    "        image_center = (image.shape[1]//2, image.shape[0]//2)\n",
    "\n",
    "        # Rotate around the image center\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(image_center, angle, 1)\n",
    "\n",
    "        # Apply the rotation to the image\n",
    "        image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "\n",
    "        # Apply the rotation to the masks\n",
    "        disc = cv2.warpAffine(disc, rotation_matrix, (disc.shape[1], disc.shape[0]))\n",
    "        fovea = cv2.warpAffine(fovea, rotation_matrix, (fovea.shape[1], fovea.shape[0]))\n",
    "\n",
    "        # Recalculate the centroid of the disc mask\n",
    "        M_disc = cv2.moments(disc)\n",
    "        if M_disc[\"m00\"] != 0:\n",
    "            disc_center_x = int(M_disc[\"m10\"] / M_disc[\"m00\"])\n",
    "            disc_center_y = int(M_disc[\"m01\"] / M_disc[\"m00\"])\n",
    "        else:\n",
    "            disc_center_x = 0\n",
    "            disc_center_y = 0\n",
    "\n",
    "        # Recalculate the centroid of the fovea mask\n",
    "        M_fovea = cv2.moments(fovea)\n",
    "        if M_fovea[\"m00\"] != 0:\n",
    "            fovea_center_x = int(M_fovea[\"m10\"] / M_fovea[\"m00\"])\n",
    "            fovea_center_y = int(M_fovea[\"m01\"] / M_fovea[\"m00\"])\n",
    "        else:\n",
    "            fovea_center_x = 0\n",
    "            fovea_center_y = 0\n",
    "\n",
    "        # Create a copy of the image to annotate\n",
    "        image_with_annotations = image.copy()\n",
    "\n",
    "        # Process and annotate disc\n",
    "        contours, _ = cv2.findContours(disc, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            if len(largest_contour) >= 5:\n",
    "                ellipse = cv2.fitEllipse(largest_contour)\n",
    "                cv2.ellipse(image_with_annotations, ellipse, (0, 255, 0), 2)\n",
    "                ellipse_center = (int(ellipse[0][0]), int(ellipse[0][1]))\n",
    "\n",
    "        # Process and annotate fovea\n",
    "        contours, _ = cv2.findContours(fovea, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        if contours:\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            M = cv2.moments(largest_contour)\n",
    "            if M[\"m00\"] != 0:\n",
    "                fovea_center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "                cv2.drawMarker(image_with_annotations, fovea_center, (255, 0, 0), cv2.MARKER_TILTED_CROSS, 20, 2)\n",
    "\n",
    "        # Draw line between ellipse center and fovea center\n",
    "        if 'ellipse_center' in locals() and 'fovea_center' in locals():\n",
    "            cv2.line(image_with_annotations, ellipse_center, fovea_center, (255, 255, 0), 2)\n",
    "\n",
    "            # Calculate and display torsion angle\n",
    "            angle_deg = calculate_torsion_angle(ellipse[2])\n",
    "            if fovea_center_x > disc_center_x:\n",
    "                angle_deg = angle_deg * -1\n",
    "            display_torsion_angle(image_with_annotations, angle_deg)\n",
    "\n",
    "        # Draw the major axis of the ellipse\n",
    "        # Calculate the endpoints of the minor axis\n",
    "        minor_axis_length = ellipse[1][1]\n",
    "        angle_of_rotation_rad_perpendicular = np.radians(ellipse[2] + 90)  \n",
    "        # Calculate the offsets from the center to the endpoints of the minor axis\n",
    "        dx_minor = (minor_axis_length / 2) * np.cos(angle_of_rotation_rad_perpendicular)\n",
    "        dy_minor = (minor_axis_length / 2) * np.sin(angle_of_rotation_rad_perpendicular)\n",
    "        # Calculate the endpoints for the minor axis\n",
    "        endpt1_minor = (int(ellipse_center[0] - dx_minor), int(ellipse_center[1] - dy_minor))\n",
    "        endpt2_minor = (int(ellipse_center[0] + dx_minor), int(ellipse_center[1] + dy_minor))\n",
    "        # Draw the minor axis line\n",
    "        cv2.line(image_with_annotations, endpt1_minor, endpt2_minor, (255, 255, 0), 2) \n",
    "        \n",
    "        start_point_vertical = (disc_center_x, 0)\n",
    "        end_point_vertical = (disc_center_x, image_with_annotations.shape[0])\n",
    "\n",
    "        # Draw the vertical meridian line\n",
    "        cv2.line(image_with_annotations, start_point_vertical, end_point_vertical, (255, 255, 0), 2) \n",
    "\n",
    "        # Ellipse ovality\n",
    "        ovality = round(ellipse[1][1] / ellipse[1][0], 2)\n",
    "        cv2.putText(image_with_annotations, f\"Ovality: {ovality}\", (image.shape[1] // 3, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)\n",
    "\n",
    "        # Display the final image\n",
    "        if display == True:\n",
    "            plt.imshow(image_with_annotations)\n",
    "            plt.title(files[i])\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        # Store results\n",
    "        filename = files[i]\n",
    "        new_row = pd.DataFrame({'Filename': [filename], 'Ovality': [ovality], 'Torsion': [angle_deg]})\n",
    "        results = pd.concat([results, new_row], ignore_index=True)\n",
    "        print(f\"Processed image {i+1}: {filename}\")\n",
    "\n",
    "        # Save the images with annotations\n",
    "        #pathOut2 = f'{pathOut}{filename}.png'\n",
    "        filename_wo_ext = os.path.splitext(filename)[0]  # Remove the existing extension\n",
    "        pathOut2 = f'{pathOut}{filename_wo_ext}.png'\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        image_with_annotations = cv2.cvtColor(image_with_annotations, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Save the image in RGB format\n",
    "        if writeImages == True:\n",
    "            cv2.imwrite(pathOut2, image_with_annotations)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {i+1}: {files[i]}\")\n",
    "        print(e)\n",
    "\n",
    "        # Store results\n",
    "        filename = files[i]\n",
    "        new_row = pd.DataFrame({'Filename': [filename], 'Ovality': 'NA', 'Torsion': 'NA'})\n",
    "        results = pd.concat([results, new_row], ignore_index=True)\n",
    "        \n",
    "        continue\n",
    "\n",
    "# Save results to a CSV file\n",
    "if writeResults == True:    \n",
    "    results.to_csv('./results/results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
